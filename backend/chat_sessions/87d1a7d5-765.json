[
  {
    "role": "system",
    "content": "You are a helpful AI assistant for the Damage Lab sensor data application.\nYou help users manage sensor data, train machine learning models for damage detection, and run inference tests.\n\n## CRITICAL: Ask Clarifying Questions\n\nWhen the user requests an action but is missing required information, ASK THEM before proceeding:\n\n1. **Training a model without architecture specified**: Ask \"Would you like to use CNN (faster, good for most cases) or ResNet (more powerful, slower)?\"\n2. **Training without a model name**: Ask what they'd like to name the model, or suggest one using `suggest_model_name()`\n3. **Ingesting data without a label**: Ask what classification label to use, or suggest one using `suggest_label()`\n4. **Running inference without specifying a model**: Ask which model to use after listing available ones\n\nDO NOT assume defaults - always confirm with the user when key parameters are missing.\n\n## Your Capabilities\n\n### Data Management\n- List and examine datasets in the database\n- Browse the file system to find new sensor data folders\n- Ingest raw sensor data with automatic preprocessing (user can provide a path like \"/home/user/data/folder\")\n- Generate AI metadata for datasets (descriptions, quality scores, training tips)\n- Delete datasets when needed\n\n### Model Training\n- List trained models and their performance metrics\n- Suggest appropriate model names based on data\n- Start training new CNN or ResNet models on selected datasets\n- Monitor training progress in real-time\n- View training reports with graphs and insights\n\n### Inference/Testing\n- Run predictions on new CSV files using trained models\n- View test history and results\n- Analyze per-chunk predictions and confidence scores\n- Organize tests with notes and tags\n\n## Workflow Guidance\n\n### Training a New Model\n1. First use `list_datasets()` to see available data\n2. **ASK the user**: Which architecture - CNN or ResNet?\n3. **ASK the user**: What name for the model? (or suggest one)\n4. Start training with `start_training()`\n5. Monitor with `get_training_status()` or wait with `wait_for_training()`\n\n### Running Inference\n1. Use `list_models()` to see available models\n2. **ASK the user**: Which model to use?\n3. Ensure the CSV file path is correct\n4. Run `run_inference()` with the model ID\n5. Review results and optionally save with tags\n\n### Ingesting New Data (when user provides a path)\nWhen user says something like \"prepare /path/to/folder\" or \"ingest data from /path/to/folder\":\n1. Use `suggest_label()` with the folder path to get a clean label\n2. **ASK the user** to confirm the suggested label\n3. Start ingestion with `ingest_data(folder_path, label)`\n4. Confirm completion and show the new dataset\n\n## Important Notes\n- Architecture options are \"CNN\" (faster, ~15-20 seconds) or \"ResNet\" (more powerful, ~30-60 seconds)\n- Training runs in the background - use status tools to monitor\n- Always verify paths exist before ingesting or running inference\n- Test results can be filtered by model name or tags\n- Reports are auto-generated during training and saved as PDFs\n- When user provides a file/folder path, use it directly for ingestion or inference\n\nBe helpful, concise, and guide users through the appropriate workflows.\nWhen errors occur, explain what went wrong and suggest solutions.\nALWAYS ask clarifying questions when required parameters are missing rather than assuming."
  },
  {
    "role": "user",
    "content": "Give me guidance on the training workflow"
  },
  {
    "role": "assistant",
    "content": null,
    "tool_calls": [
      {
        "id": "call_nzm9vwEbsqNFE7BcamvmVR8e",
        "type": "function",
        "function": {
          "name": "get_workflow_guidance",
          "arguments": "{\"workflow\":\"training\"}"
        }
      }
    ]
  },
  {
    "role": "tool",
    "tool_call_id": "call_nzm9vwEbsqNFE7BcamvmVR8e",
    "content": "{\n  \"status\": \"success\",\n  \"guidance\": {\n    \"title\": \"Model Training Workflow\",\n    \"steps\": [\n      \"1. Check available datasets with list_datasets()\",\n      \"2. Review dataset details to ensure quality with get_dataset_details(label_id)\",\n      \"3. Generate AI metadata if needed with generate_dataset_metadata(label_id)\",\n      \"4. Get a suggested model name with suggest_model_name(labels, architecture)\",\n      \"5. Start training with start_training(model_name, labels, architecture)\",\n      \"6. Monitor progress with get_training_status(job_id)\",\n      \"7. Review the generated report in list_reports()\"\n    ],\n    \"tips\": [\n      \"Use CNN for faster training on simpler patterns\",\n      \"Use ResNet for more complex data or when CNN underfits\",\n      \"Always check dataset quality scores before training\"\n    ]\n  }\n}"
  },
  {
    "role": "assistant",
    "content": "Here\u2019s a clear training workflow you can follow in Damage Lab, from data to trained model:\n\n---\n\n## 1. Check what data you can train on\n\nUse this to see all processed datasets/labels that are available:\n\n- `list_datasets()`\n\nWhat to look for:\n- That the labels you care about (e.g., `crushcore_0.75`, `disbond_1.0`) are present.\n- That you have all classes you want to distinguish (e.g., \u201chealthy\u201d vs \u201cdamaged\u201d types).\n\nIf you\u2019d like, I can run `list_datasets()` and walk through what\u2019s there with you.\n\n---\n\n## 2. Inspect dataset quality (recommended)\n\nFor any candidate label/dataset:\n\n- `get_dataset_details(label_id)`\n\nCheck:\n- Number of samples / chunks per class\n- Any imbalance (e.g., far more healthy than damage)\n- Sampling rate, chunk duration, etc.\n\nIf you want deeper AI-generated guidance for a dataset:\n\n- `generate_dataset_metadata(label_id)`\n\nThis will give:\n- A human-readable description of the data\n- A quality score\n- Training tips (e.g., \u201cadd more examples of class X\u201d, \u201cimbalanced classes\u201d, etc.)\n\n---\n\n## 3. Decide which labels to train on\n\nPractical rules:\n- Start simple: 2\u20133 classes (e.g., `healthy`, `crushcore_0.75`, `disbond_1.0`).\n- Avoid classes with extremely few samples at first.\n- Make sure each class has at least a few dozen chunks if possible.\n\nOnce chosen, you\u2019ll have something like:\n\n```text\nlabels = [\"healthy\", \"crushcore_0.75\", \"disbond_1.0\"]\n```\n\nIf you tell me which damage types you care about, I can suggest a label set.\n\n---\n\n## 4. Choose an architecture (you must decide)\n\nYou need to pick one before training:\n\n- **CNN**\n  - Faster (~15\u201320 seconds)\n  - Good default for most use cases\n  - Try this first if you\u2019re exploring or iterating quickly\n\n- **ResNet**\n  - More powerful, deeper network\n  - Slower (~30\u201360 seconds)\n  - Better when patterns are complex or CNN clearly underfits\n\nIf you\u2019re unsure, I\u2019d start with **CNN** and only move to ResNet if performance is not acceptable.\n\n---\n\n## 5. Pick a model name\n\nYou can:\n- Manually choose something meaningful, e.g.:\n  - `cnn_crushcore_disbond_v1`\n  - `resnet_multi_damage_2025_01`\n- Or get an AI-suggested name with:\n\n  - `suggest_model_name(labels, architecture)`\n\nModel naming tips:\n- Include architecture (`cnn` / `resnet`)\n- Reference key labels (e.g., `crushcore`, `disbond`)\n- Optionally add version or date\n\n---\n\n## 6. Start training\n\nOnce you know:\n- `labels` (list of dataset labels)\n- `architecture` (\"CNN\" or \"ResNet\")\n- `model_name` (string)\n\nTrain with:\n\n- `start_training(model_name, labels, architecture, generate_report=True, use_llm=True)`\n\nThis will:\n- Train the network\n- Generate a PDF training report\n- Use AI to add insights into the report (if `use_llm=True`)\n\n---\n\n## 7. Monitor training progress\n\nYou get back a `job_id` from `start_training()`.\n\nUse:\n- `get_training_status(job_id)` to poll for status, or\n- `wait_for_training(job_id, poll_interval, max_wait)` to block until finished\n\nStatus typically shows:\n- Current epoch / total epochs\n- Training/validation accuracy\n- Loss values\n- Whether the job completed or failed\n\n---\n\n## 8. Review training results and report\n\nWhen training is done:\n\n1. Check the model in the list:\n   - `list_models()`\n   - Optionally: `get_model_details(model_id)` for deeper info (accuracy, confusion matrix, etc.)\n\n2. Open the training report:\n   - `list_reports()` to see available reports\n   - Then open/download the PDF you\u2019re interested in from the system UI (or however your environment presents it)\n\nWhat to look for:\n- Overall accuracy and per-class accuracy\n- Confusion matrix (which classes get confused)\n- Loss curves (overfitting/underfitting signs)\n- Any AI-generated recommendations in the report\n\n---\n\n## 9. Iterate and improve (optional but common)\n\nBased on results, you might:\n- **Data changes**\n  - Add more data to underperforming classes\n  - Remove problematic/noisy datasets\n- **Model changes**\n  - Try switching CNN \u2194 ResNet\n  - Retrain with different label sets (e.g., merge similar classes)\n- **Versioning**\n  - Name new runs clearly, e.g. `cnn_crushcore_disbond_v2_balanced`\n\n---\n\n## How I can help you next\n\nIf you\u2019d like to actually start training now, I need three things from you:\n\n1. Which dataset labels to train on (I can list them for you with `list_datasets()`).\n2. Which architecture you prefer: **CNN (faster)** or **ResNet (more powerful)**?\n3. What you want to name the model (or I can suggest one via `suggest_model_name()`).\n\nTell me these, or say \u201chelp me choose labels/architecture/name\u201d and I\u2019ll walk you through the choices interactively."
  }
]